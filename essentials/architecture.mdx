---
title: "System Architecture"
description: "A deep dive into the backend architecture, core concepts, data models, and development patterns that power the Open Politics HQ platform."
---

## Core Philosophy

The Open Politics HQ platform is engineered to perform **qualitative research at a quantitative scale**. It achieves this by operationalizing complex, human-centric analysis into reproducible and automated workflows.

<CardGroup cols={2}>
    <Card title="Auditability & Reproducibility" icon="clipboard-check">
        All analysis is fully traceable through immutable `AnnotationRun` records, ensuring that every result can be verified and reproduced.
    </Card>
    <Card title="Schema-Driven Analysis" icon="atom">
        Every AI analysis task is governed by an `AnnotationSchema`, a powerful contract that uses natural language to define a structured JSON output.
    </Card>
    <Card title="Composable Services" icon="network-wired">
        The backend is built on a decoupled, service-oriented architecture, allowing for the creation of complex workflows by chaining services together.
    </Card>
    <Card title="Asynchronous by Default" icon="tasks">
        Heavy operations like content ingestion and AI analysis are handled by background workers, ensuring the system remains responsive and scalable.
    </Card>
</CardGroup>

---

## The Intelligence Workflow

The platform transforms unstructured, multi-modal content into structured, auditable intelligence through a three-stage process: Ingestion, Analysis, and Curation.

```mermaid
flowchart TD
    subgraph Ingestion["1. Content Ingestion"]
        A["Source (URL, File, Search Query)"] --> B[ContentIngestionService];
        B --> C[Asset Creation & Processing];
    end
    
    subgraph Analysis["2. Structured Analysis"]
        D["AnnotationSchema (The 'How')"] --> E[AnnotationRun];
        C --> E["Asset(s) (The 'What')"];
        E --> F[Annotation (The Structured Result)];
    end
    
    subgraph Curation["3. Curation & Synthesis"]
        F --> G[Justification (The 'Why')];
        F --> H[Intelligence Fragment];
        H --> C;
    end
```

---

## Core Concepts & Data Model

Understanding these key entities is fundamental to using and developing on the platform.

<AccordionGroup>
<Accordion title="Infospace">
  An **Infospace** is the top-level boundary for a project, acting as a secure, multi-tenant container. All data, including assets, schemas, and analysis results, is scoped to a single `Infospace`.
</Accordion>

<Accordion title="Asset">
  An **Asset** is an immutable record of ingested content. It's the fundamental unit of data in the system.

  - **Multimodality**: Assets are designed to be multi-modal from the ground up. An `Asset` can be a PDF, a web page, a text file, an image, or even an audio file.
  - **Parent-Child Relationships**: The system automatically creates asset hierarchies. For example, ingesting a PDF creates a parent `Asset` for the document and child `Assets` for each page. Similarly, a web article will have child `Assets` for its embedded images. This structure is crucial for fine-grained, multi-modal analysis.
  
  ```json
  {
    "id": 101,
    "kind": "PDF",
    "title": "Annual Climate Report 2024.pdf",
    "parent_asset_id": null,
    "children": [
      { "id": 102, "kind": "PDF_PAGE", "title": "Page 1" },
      { "id": 103, "kind": "PDF_PAGE", "title": "Page 2" }
    ]
  }
  ```

</Accordion>
<Image src="/images/schema-polcat.png" alt="Asset" />

<Accordion title="AnnotationSchema">
  The **AnnotationSchema** is the blueprint for an AI analysis task. This is where you turn natural language instructions into a structured data extraction engine.

  - **Natural Language Instructions**: You define what the AI should find using clear, descriptive text.
  - **JSON Output Contract**: You provide a JSON schema that defines the desired structure of the output. The AI will conform its response to this schema.
  - **LLM Agnostic**: The schema works with any configured LLM, including local models via **Ollama**.
  
  <Tip>
    This schema-driven approach is the core of the platform's power, enabling you to create bespoke, reusable analysis tools for any domain.
  </Tip>

  <CodeGroup>
  ```json AnnotationSchema Example
  {
    "name": "Identify Key Policy Points",
    "instructions": "Read the following document and identify the key policy recommendations. For each recommendation, extract the main point, the stakeholders involved, and the proposed timeline.",
    "output_contract": {
      "type": "object",
      "properties": {
        "policy_points": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "recommendation": { "type": "string" },
              "stakeholders": { "type": "array", "items": { "type": "string" } },
              "timeline": { "type": "string" }
            }
          }
        }
      }
    }
  }
  ```
  </CodeGroup>
</Accordion>

<Accordion title="AnnotationRun & Annotation">
  An **AnnotationRun** is an immutable record that captures the execution of one or more `AnnotationSchemas` against a set of `Assets`. It is the primary unit of auditability.

  - **Inputs**: A run links the specific `Assets` and `Schemas` that were used.
  - **Configuration**: It stores the exact configuration, including the AI model used.
  
  An **Annotation** is the result of this process: a single structured JSON object that applies one `Schema` to one `Asset` within a `Run`.
</Accordion>

<Accordion title="Justification & Intelligence Fragment">
  A **Justification** is an optional record that provides the "why" behind an `Annotation`. It captures the AI's reasoning and the specific evidence from the source `Asset` (like text spans or image regions) that it used to generate the result.

  An **Intelligence Fragment** is a piece of curated information that has been promoted from a transient `Annotation` to become a permanent, queryable fact stored on an `Asset`'s metadata. This is how you build a structured knowledge base over time.
</Accordion>

</AccordionGroup>

---

## Service Layer Architecture

The backend is composed of a set of services with clear, distinct responsibilities, promoting modularity and ease of maintenance.

```mermaid
graph TD
    subgraph Unified Entry Points
        A["ContentIngestionService"]
        B["AnnotationService"]
        C["IntelligenceConversationService"]
    end

    subgraph Core Services
        D["AssetService"]
        E["BundleService"]
        F["SourceService"]
        G["TaskService"]
        H["PipelineService"]
        I["MonitorService"]
    end
    
    subgraph Provider Abstractions
        J["ModelRegistryService"]
        K["StorageProvider"]
        L["SearchProvider"]
        M["ScrapingProvider"]
    end
    
    A --> D
    A --> F
    A --> L
    A --> M
    B --> D
    B --> J
    C --> B
    C --> A
    H --> B
    I --> B
    I --> G
end
```

### Primary Services

-   **`ContentIngestionService`**: The single entry point for all content. It handles discovery (from search, URLs), ingestion (files, text), and processing (scraping, parsing), creating the corresponding `Asset` records.
-   **`AnnotationService`**: Manages all AI analysis. It creates `AnnotationRun`s, processes them via the `ModelRegistryService`, and stores the results.
-   **`IntelligenceConversationService`**: Powers the chat interface and orchestrates AI tool calling.

### AI Tool Calling

The `IntelligenceConversationService` enables LLMs to act as autonomous agents. Models can call tools to interact with the platform's services, allowing them to perform complex, multi-step tasks.

**Example Use Case: AI Research Assistant**
1.  A user asks: "Find recent articles about AI policy and summarize their key findings."
2.  The LLM, via the `IntelligenceConversationService`, calls the `search_assets` tool with the query "AI policy".
3.  The service returns a list of relevant `Assets`.
4.  The LLM then calls the `analyze_assets` tool, passing in the asset IDs and the ID of a "Summarization" `AnnotationSchema`.
5.  This triggers a new `AnnotationRun`. Once complete, the LLM retrieves the results.
6.  Finally, the LLM synthesizes the summaries and presents a consolidated answer to the user.

<Info>
  The platform supports a variety of LLM providers, including OpenAI, Gemini, and local models served through **Ollama**, which can all be used for tool calling.
</Info>

---

## Automation and Extensibility

The platform is designed to be extended and automated.

-   **Automation Patterns**: Use **Monitors** for simple, continuous analysis of new assets, or **Pipelines** for complex, multi-stage conditional workflows.
-   **Extending the Platform**:
    -   **Add a Content Source**: Extend `ContentIngestionService` to handle a new data source.
    -   **Add an AI Analysis**: Create a new `AnalysisAdapter` for a custom, non-LLM analysis task.
    -   **Add an LLM Provider**: Implement the `LanguageModelProvider` interface to integrate a new model provider.

---

## Performance & Scalability

-   **Provider Caching**: Reduces overhead by caching connections to external services like AI models and storage.
-   **Data Pre-fetching & Batching**: Minimizes database queries during large analysis runs.
-   **Concurrent Processing**: Uses `asyncio` to execute tasks in parallel while respecting API rate limits.
-   **Asynchronous Operations**: Ensures the API remains responsive during I/O-bound tasks like web scraping.
