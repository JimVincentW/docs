---
title: 'Scraping Service'
description: 'Scraper service'
icon: 'spider'
---

import ScraperDev from '/snippets/scraper-dev.mdx';

<Note>
    This setup is intended to be temporary and flexible. You can insert new scrapers by adding them to the `scraper_config.py` file.
    They can be simple python sourcing scripts loading something from an S3 Bucket or more complex ones scraping websites.
</Note>



The scraper service executes a flow of tasks to collect and process content. Here's how it works:

<Steps>
    <Step title="Initialize Scraping Flow">
        1. Service receives scraping flags (e.g., "cnn", "reuters"). They are essentially "Jobs" that need to be executed.
    </Step>

    <Step title="Execute Source Scripts" icon="code" iconType="solid">
        For each source flag:
        ```python SSARE/scraper_service/scrapers/scraper_config.py
        # Load source configuration
        config = {
            "scrapers": {
                "cnn": {
                    "location": "scrapers/cnn.py",
                    "last_run": "2015-01-01 00:00:00"
                }
            }
        }
        # Execute corresponding scraper script
        # Script outputs to CSV: url | headline | paragraphs
        ```
        For example this script returning cnn articles:
        ```python SSARE/scraper_service/scrapers/cnn.py
        articles = await asyncio.gather(*tasks)
        return pd.DataFrame(articles, columns=['url', 'headline', 'paragraphs'])
        ....
        async def main():
            async with aiohttp.ClientSession() as session:
                df = await scrape_cnn_articles(session)
                os.makedirs('/app/scrapers/data/dataframes', exist_ok=True)

                df.to_csv('/app/scrapers/data/dataframes/cnn_articles.csv', index=False)
        ```

        <Note>
            The time function of regular scraping is not yet implemented. As it stands now all flags are always set, and we just run them every time.
            Our Orchestration.py script is responsible for that.
    </Step>

    <Step title="Process Results" icon="gears" iconType="solid">
        For each successful scrape:
        1. Load CSV data into pandas DataFrame
        2. Convert records to Content models:
        ```python
        Content(
            url=content_data['url'],
            title=content_data['headline'],
            text_content=content_data['paragraphs'],
            source=flag,
            content_type="article",
            insertion_date=datetime.utcnow()
        )
        ```
    </Step>

    {/* <Step title="Queue for Processing" icon="database" iconType="solid">
        1. Push processed content to Redis queue
        2. Set 'scrapers_running' flag to '0'
        3. Close Redis connection
    </Step> */}
</Steps>

<ScraperDev />

