---
title: 'Data & Our Engine'
description: 'SSARE provides a ready-to-use infrastructure for systematically ingesting, processing, and hosting data to meet your analytical needs. Our open-source service offers a robust data backbone for political intelligence and news analysis.'
---

## Introduction
SSARE is an open-source service designed to orchestrate the collection, processing, and analysis of news articles, with a focus on political intelligence.
<Note>
SSARE stands for **S**emantic **S**earch **A**rticle **R**ecommendation **E**ngine.
</Note>
### Key Features:
<CardGroup cols={2}>
  <Card title="Scraping" icon="spider-web">
    Ingest data from arbitrary sourcing scripts
  </Card>
  <Card title="Vector Processing" icon="vector-square">
    Convert articles into vector representations
  </Card>
  <Card title="Entity Recognition" icon="tag">
    Identify entities like locations, persons, and organizations
  </Card>
  <Card title="Geocoding" icon="map-pin">
    Convert recognized locations to geographical coordinates
  </Card>
  <Card title="Storage" icon="database">
    Store articles and metadata in SQL and vector databases
  </Card>
  <Card title="Querying" icon="magnifying-glass">
    Provide endpoints for semantic search and recommendations
  </Card>
  <Card title="LLM Classification" icon="robot">
    Use natural language models to organize, label, and rate data
  </Card>
  <Card title="Local LLM Support" icon="microchip">
    Integrate with Ollama for on-premise LLM processing
  </Card>
  <Card title="Structured Output" icon="code">
    Leverage Instructor for generating structured data from LLMs
  </Card>
  <Card title="Orchestration" icon="diagram-project">
    Manage and schedule tasks efficiently using Prefect, a workflow orchestration and observation tool.
    It can be observed in the SSARE Dashboard.
  </Card>
</CardGroup>


## Quick Start
1. Get SSARE up and running in minutes:
```bash bash
git clone https://github.com/open-politics/SSARE.git
cd SSARE
mv .env.example .env
docker-compose up --build
```

Access the dashboard at 
````bash bash 
http://localhost:8089/
````

## Architecture

<AccordionGroup>
  <Accordion icon="broom" title="Scraper Service">
    Handles the ingestion of news articles from various sources.
  </Accordion>
  <Accordion icon="brain" title="Vectorization/NLP Service">
    Processes text into vector representations for semantic analysis.
    If you want use a different model you need to adapt the models vector in SSARE/core/models.py
  </Accordion>
  <Accordion icon="database" title="PostgreSQL Service">
    - Stores the full text and metadata of articles.
    - Responsible for creating jobs and storing processed data.
    - Querying the database with [SQLModel](https://sqlmodel.tiangolo.com/)
  </Accordion>
  <Accordion icon="tag" title="Entity Service">
    Performs Named Entity Recognition on article text.
    Read more about Named Entity Recognition [here](/essentials/ner)
  </Accordion>
  <Accordion icon="map-pin" title="Geocoding Service">
    Converts location entities to geographical coordinates.
    When you start up the services first the Pelias Docker Image will download a placeholder dataset.
        This means you can fully locally convert locations to geocoordinates and in general handle locations and region mapping without any external service.
  </Accordion>
  <Accordion icon="server" title="API Service">
    
  </Accordion>
</AccordionGroup>

##  Flows

### Ingestion Process
<Card title='Ingestion Process'>
<Steps>
  <Step title="Collect Articles" icon="newspaper" iconType="solid">
    Scraper scripts collect articles from various sources.
  </Step>
  <Step title="Process Articles" icon="fa-solid fa-gears" iconType="solid">
    Articles are processed for vector representation and entity extraction.
    <Steps>
      <Step title="Vector Representation" icon="vector-square" iconType="solid">
        Convert article text into numerical vectors for semantic analysis.
      </Step>
      <Step title="Entity Extraction" icon="tags" iconType="solid">
        Identify and extract named entities such as persons, organizations, and locations.
      </Step>
      <Step title="Geocoding" icon="map" iconType="solid">
        Convert extracted location entities to geographical coordinates.
      </Step>
      <Step title="Classification" icon="sitemap" iconType="solid">
        LLMs are used to classify the article into the categories you want to use.
      </Step>
    </Steps>
  </Step>
  <Step title="Store Data" icon="database" iconType="solid">
    Processed data is stored in PostgreSQL and vector databases.
  </Step>
  <Step title="Provide Access" icon="server" iconType="solid">
    API endpoints allow for querying and retrieval of processed data.
  </Step>
</Steps>
</Card>

### Adding Sources
<Card>
<Steps>
  <Step title="Create Scraping/ Sourcing Script">
    To add a new source to SSARE, create a scraping script in the scraper_service/scrapers folder.
    That doesn't necessarily need to be scraping script, it can also be just a function where you load data e.g. from an S3 Storage Bucket.
  </Step>
  <Step title="Define Output">
    Ensure your script outputs a DataFrame with the following columns:

    > | url | headline | paragraphs | source |

    <CodeGroup>
```python Example Scraper
import requests
from bs4 import BeautifulSoup
import pandas as pd
def scrape_cnn_articles():
# Scraping logic here
# Save with the following columns:
# url
# headline
# paragraphs
# source
# ...
df = pd.DataFrame(articles)
return df
cnn_articles_df = scrape_cnn_articles()
cnn_articles_df.to_csv('cnn_articles.csv', index=False)
```
</CodeGroup>
  </Step>
  <Step title="Integrate with SSARE">
    Add your new scraper to the SSARE pipeline for automatic processing.
  </Step>
</Steps>
</Card>




## Use Cases

### Entity Ranking

Retrieve and rank entities from your processed articles:

<CodeGroup>
```python Entity Ranking Script
import requests
from collections import Counter, defaultdict

def print_sorted_entities(entity_type, limit=10):
    url = 'http://localhost:5434/articles'
    params = {
        'geocoding_created': 0,
        'limit': 200,
        'embeddings_created': 1,
        'entities_extracted': 1
    }

    response = requests.get(url, params=params)
    if response.status_code == 200:
        data = response.json()
        entity_counter = Counter()
        entity_articles = defaultdict(list)

        for article in data:
            entities = article['entities']
            for entity in entities:
                if entity['tag'] == entity_type:
                    entity_name = entity['text']
                    entity_counter[entity_name] += 1
                    entity_articles[entity_name].append(article['headline'] or article['url'])

        for entity, count in entity_counter.most_common(limit):
            print(f"{entity_type}: {entity}, Count: {count}")
            print("Associated Articles:")
            for article in set(entity_articles[entity]):
                print(f" - {article}")
            print("\n")
    else:
        print('API request failed.')

print_sorted_entities('NORP', 5)  # Print top 5 NORP (affiliation) entities
```
</CodeGroup>

### GeoJSON Generation

Create GeoJSON features from the locations in your data:

<CodeGroup>
```python GeoJSON Generation
import requests
import json
def generate_geojson():
url = 'http://localhost:5434/articles'
params = {
'geocoding_created': 1,
'limit': 100
}
response = requests.get(url, params=params)
if response.status_code == 200:
    data = response.json()
    features = []

    for article in data:
        for entity in article['entities']:
            if entity['tag'] == 'GPE' and 'lat' in entity and 'lon' in entity:
                feature = {
                    "type": "Feature",
                    "geometry": {
                        "type": "Point",
                        "coordinates": [entity['lon'], entity['lat']]
                    },
                    "properties": {
                        "name": entity['text'],
                        "article": article['headline']
                        # add any other information you want to include in the geojson
                    }
                }
                features.append(feature)

    geojson = {
        "type": "FeatureCollection",
        "features": features
    }

    with open('locations.geojson', 'w') as f:
        json.dump(geojson, f)

    print("GeoJSON file created: locations.geojson")
else:
    print('API request failed.')
generate_geojson()
```
</CodeGroup>

## Roadmap

<CardGroup cols={2}>
  <Card title="Custom Embedding Models" icon="cube">
    Support for user-defined embedding models
  </Card>
  <Card title="Enhanced Geocoding" icon="map">
    Improve accuracy and coverage of location data
  </Card>
  <Card title="Kubernetes Orchestration" icon="ship">
    Scalable deployment with Kubernetes
  </Card>
  <Card title="Expanded Scraper Support" icon="spider">
    We are looking forward to creating "flavors" of information spaces. This will need flexible and modular scraping.
  </Card>
  <Card title="Knowledge Graphs" icon="diagram-project">
    Implement knowledge graph capabilities for enhanced data relationships
  </Card>
  <Card title="GraphRAG" icon="chart-network">
    Integrate Graph Retrieval-Augmented Generation for improved context understanding
  </Card>
  <Card title="Custom Information Spaces" icon="globe">
    Our initial focus is on international politics and global news. 
    We aim to expand this to individual information spaces for more granular coverage. 
    We're also working towards multi-region and multi-language support.
  </Card>
</CardGroup>

## Contributing

We welcome contributions from developers, data scientists, and political enthusiasts. To contribute:

1. Fork the repository
2. Create a new branch for your feature
3. Commit your changes
4. Open a pull request

For major changes, please open an issue first to discuss what you would like to change.

## License

SSARE is distributed under the MIT License. See the `LICENSE` file in the repository for full details.
