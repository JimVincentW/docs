---
title: 'Data & Our Engine'
description: 'SSARE provides a ready-to-use infrastructure for systematically ingesting, processing, and hosting data to meet your analytical needs. Our open-source service offers a robust data backbone for political intelligence and news analysis.'
---

## Introduction
SSARE is an open-source service designed to orchestrate the collection, processing, and analysis of news articles, with a focus on political intelligence.
<Note>
SSARE stands for **S**emantic **S**earch **A**rticle **R**ecommendation **E**ngine.
</Note>
### Key Features:
<CardGroup cols={2}>
  <Card title="Scraping" icon="spider-web">
    Ingest data from arbitrary sourcing scripts
  </Card>
  <Card title="Vector Processing" icon="vector-square">
    Convert articles into vector representations
  </Card>
  <Card title="Entity Recognition" icon="tag">
    Identify entities like locations, persons, and organizations
  </Card>
  <Card title="Geocoding" icon="map-pin">
    Convert recognized locations to geographical coordinates
  </Card>
  <Card title="Storage" icon="database">
    Store articles and metadata in SQL and vector databases
  </Card>
  <Card title="Querying" icon="magnifying-glass">
    Provide endpoints for semantic search and recommendations
  </Card>
  <Card title="LLM Classification" icon="robot">
    Use natural language models to organize, label, and rate data
  </Card>
  <Card title="Local LLM Support" icon="microchip">
    Integrate with Ollama for on-premise LLM processing
  </Card>
  <Card title="Structured Output" icon="code">
    Leverage Instructor for generating structured data from LLMs
  </Card>
  <Card title="Orchestration" icon="diagram-project">
    Manage and schedule tasks efficiently using Prefect, a workflow orchestration and observation tool.
    It can be observed in the SSARE Dashboard.
  </Card>
</CardGroup>


## Quick Start
1. Get SSARE up and running in minutes:
```bash bash
git clone https://github.com/open-politics/SSARE.git
cd SSARE
mv .env.example .env
docker-compose up --build
```

Access the dashboard at 
````bash bash 
http://localhost:8089/
````

## Architecture

Our data engine consists out of several pipelines packaged in individual docker instances.
Each of these act like a serverles or ETL workflow. They engineer new features upon the base doc format.
This aims to generally enable more and more features. The data validation is completely done with Pydantic. THe queris with SQLModel.
So if you change the code/models.py (and start with a fresh db) you can start adding custom fields to your data.
Add a new service, change the models.py and orchestration.py.
Add the trigger and saving operations in the ssare/postgres_service/main.py
### Orchestration & observation
#### Prefect
<Accordion title="Prefect Dashboard">
  Currently the docker containers decorate tasks and flows with Prefect.
  This means that you can see all the tasks and flows in the dashboard.
  
  Furthermore this should enable converting the flows from http based triggers to prefect deployments
  that can be e.g. deployed in a Ray Cluster.
  <Accordion title="Prefect Tasks & Flows">
  <CodeGroup>
  ````bash prefect 
  from prefect import flow, task


  @task
  def say_hello(name: str):
      print(f"Hello {name}!")


  @flow
  def hello_universe(names: list[str]):
      for name in names:
          say_hello(name)


  if __name__ == "__main__":
      hello_universe.serve(name="your-first-deployment")
  ````

   ````bash w/ Ray 
  from prefect import flow, task
  # you need to install prefect-ray with pip in the container
  from prefect_ray.task_runners import RayTaskRunner

  @task
  def say_hello(name: str):
      print(f"Hello {name}!")


  @flow(task_runner=RayTaskRunner()) # insert address, but check the Prefect documenation for more infos
  def hello_universe(names: list[str]):
      for name in names:
          say_hello(name)


  if __name__ == "__main__":
      hello_universe.serve(name="your-first-deployment")
  ````
  </CodeGroup>
  </Accordion>
<Frame>
  <img src="/images/prefect_dashboard.png" />
</Frame>
</Accordion>
### Services

<AccordionGroup>
  <Accordion icon="broom" title="Scraper Service">
    Handles the ingestion of news articles from various sources.
  </Accordion>
  <Accordion icon="brain" title="Vectorization/NLP Service">
    Processes text into vector representations for semantic analysis.
    If you want use a different model you need to adapt the models vector in SSARE/core/models.py
  </Accordion>
  <Accordion icon="database" title="PostgreSQL Service">
    - Stores the full text and metadata of articles.
    - Responsible for creating jobs and storing processed data.
    - Querying the database with [SQLModel](https://sqlmodel.tiangolo.com/)
  </Accordion>
  <Accordion icon="tag" title="Entity Service">
    Performs Named Entity Recognition on article text.
    Read more about Named Entity Recognition [here](/essentials/ner)
  </Accordion>
  <Accordion icon="map-pin" title="Geocoding Service">
    Converts location entities to geographical coordinates.
    When you start up the services first the Pelias Docker Image will download a placeholder dataset.
        This means you can fully locally convert locations to geocoordinates and in general handle locations and region mapping without any external service.
  </Accordion>
  <Accordion icon="server" title="Postgres Service">
    Quries Postgres Database. Relational, text search and vector search.
    Creates jobs for pipelines.
    Stores articles. 
  </Accordion>
</AccordionGroup>
### Databases
<AccordionGroup>
  <Accordion icon="broom" title="Relational Databases">
    - PostgreSQL (with PGVector Plugin)  
    -- ArticlesDB: stores the full text and metadata of articles. Entities, Locations, Classifications
    -- PrefectDB: stores the workflow state and configuration.
  </Accordion>
  <Accordion icon="broom" title="Vector Databases">
    - PGVector
    - Qdrant (re-introducing soon)
  </Accordion>
  <Accordion icon="broom" title="Graph Databases">
    - Neo4j (upcoming): for all things graphs
  </Accordion>
</AccordionGroup>

##  Flows

### Ingestion Process
<Card title='Ingestion Process'>
<Steps>
  <Step title="Collect Articles" icon="newspaper" iconType="solid">
    Scraper scripts collect articles from various sources.
    Sources can be adjusted with custom source scripts, just make them return:
    > url, headline, paragraphs and source in a csv file.

    This flexibility allows you to integrate various data sources into the SSARE pipeline.
  </Step>
  <Step title="Process Articles" icon="fa-solid fa-gears" iconType="solid">
    Articles are processed for vector representation and entity extraction.
    Each of the following steps is a microservice roughly working in the same way:
    {/* <img src='/images/service_pipes.png'/ > */}
    <Steps>
      <Step title="Vector Representation" icon="vector-square" iconType="solid">
        Convert article text into numerical vectors for semantic analysis.
      </Step>
      <Step title="Entity Extraction" icon="tags" iconType="solid">
        Identify and extract named entities such as persons, organizations, and locations.
      </Step>
      <Step title="Geocoding" icon="map" iconType="solid">
        Convert extracted location entities to geographical coordinates.
      </Step>
      <Step title="Classification" icon="sitemap" iconType="solid">
        LLMs are used to classify the article into the categories you want to use.
        With instructor you can choose any dimension that you wish to analyse your data by.
        You can use str, int, List[str] and most other regular types supported by Pydantic models to define your classification schema. This flexibility allows you to tailor the classification to your specific needs and extract structured information from the articles efficiently.

        <Info>You can use local llms with Ollama through SSARE, just change LOCAL_LLM=True in the .env.</Info>

        <Accordion title="Two possible solutions for custom intelligence:">

          
        <CodeGroup>
          ```python Political Bias Analysis
          import instructor
          from openai import OpenAI
          from pydantic import BaseModel
          from typing import List, Dict

          class PoliticalBiasMetrics(BaseModel):
              overall_bias: str  # e.g., "left", "center-left", "center", "center-right", "right"
              bias_score: float  # Scale from -1.0 (far left) to 1.0 (far right)
              partisan_phrases: List[str]
              balanced_reporting_score: float  # Scale from 0.0 (highly biased) to 1.0 (perfectly balanced)
              key_topics: List[str]
              topic_bias: Dict[str, float]  # e.g., {"economy": 0.2, "healthcare": -0.3}

          # Patch the OpenAI client with instructor
          client = instructor.patch(OpenAI())

          # Extract political bias metrics from an article
          bias_metrics = client.chat.completions.create(
              model="gpt-4o",
              response_model=PoliticalBiasMetrics,
              messages=[
                  {"role": "system", "content": "You are an expert political analyst tasked with objectively assessing political bias in news articles. Provide a detailed, impartial analysis based solely on the content provided."},
                  {"role": "user", "content": "Analyze the political bias in the following article, focusing on overall bias, specific partisan language, balanced reporting, key topics, and topic-specific biases. Provide your analysis in a structured format.\n\nArticle text: [Insert full article text here]"}
              ],
          )

          print(f"Overall bias: {bias_metrics.overall_bias}")
          print(f"Bias score: {bias_metrics.bias_score}")
          print(f"Partisan phrases: {', '.join(bias_metrics.partisan_phrases)}")
          print(f"Balanced reporting score: {bias_metrics.balanced_reporting_score}")
          print(f"Key topics: {', '.join(bias_metrics.key_topics)}")
          print(f"Topic-specific biases: {bias_metrics.topic_bias}")
          ```

          ```python Environmental Impact Assessment
          import instructor
          from openai import OpenAI
          from pydantic import BaseModel
          from typing import List, Dict

          class EnvironmentalImpactMetrics(BaseModel):
              primary_environmental_topics: List[str]
              topic_relevance: Dict[str, float]  # e.g., {"climate_change": 0.9, "biodiversity": 0.5}
              sentiment: Dict[str, float]  # e.g., {"climate_change": -0.7, "renewable_energy": 0.8}
              proposed_solutions: List[Dict[str, str]]  # e.g., [{"solution": "Carbon tax", "topic": "Climate change"}]
              urgency_score: float  # Scale from 0.0 (low urgency) to 1.0 (high urgency)
              scientific_accuracy: float  # Scale from 0.0 (low accuracy) to 1.0 (high accuracy)
              key_stakeholders: List[str]

          # Patch the OpenAI client with instructor
          client = instructor.patch(OpenAI())

          # Assess environmental impact metrics from an article
          impact_metrics = client.chat.completions.create(
              model="gpt-4",
              response_model=EnvironmentalImpactMetrics,
              messages=[
                  {"role": "system", "content": "You are an expert environmental scientist tasked with analyzing news articles for their environmental content and impact. Provide a comprehensive, objective assessment based on the latest scientific understanding."},
                  {"role": "user", "content": "Analyze the environmental impact discussed in the following article. Focus on identifying key topics, assessing their relevance and sentiment, evaluating proposed solutions, determining the urgency of the issues, checking scientific accuracy, and identifying key stakeholders. Provide your analysis in a structured format.\n\nArticle text: [Insert full article text here]"}
              ],
          )

          print(f"Primary environmental topics: {', '.join(impact_metrics.primary_environmental_topics)}")
          print(f"Topic relevance: {impact_metrics.topic_relevance}")
          print(f"Sentiment analysis: {impact_metrics.sentiment}")
          print(f"Proposed solutions: {', '.join([f'{s['solution']} ({s['topic']})' for s in impact_metrics.proposed_solutions])}")
          print(f"Urgency score: {impact_metrics.urgency_score}")
          print(f"Scientific accuracy: {impact_metrics.scientific_accuracy}")
          print(f"Key stakeholders: {', '.join(impact_metrics.key_stakeholders)}")
          ```
          </CodeGroup>
        </Accordion>

        These examples demonstrate how you can create custom intelligence systems to extract specific metrics from articles, tailored to your analysis needs.
      </Step>
    </Steps>
  </Step>
  <Step title="Store Data" icon="database" iconType="solid">
    Processed data is stored in PostgreSQL and vector databases.
  </Step>
  <Step title="Access" icon="server" iconType="solid">
    API endpoints allow for querying and retrieval of processed data.
    Here's a detailed breakdown of the API endpoint for retrieving articles:

    ### Get Articles
    <Accordion title="Query Parameters">
    <ParamField query="url" type="string">
      Optional URL to filter articles by specific URL.
    </ParamField>

    <ParamField query="search_query" type="string">
      Optional search query to filter articles based on content.
    </ParamField>

    <ParamField query="search_type" type="string" default="TEXT">
      Type of search to perform. Can be either "TEXT" for standard text search or "SEMANTIC" for embedding-based search.
    </ParamField>

    <ParamField query="skip" type="integer" default="0">
      Number of articles to skip (for pagination).
    </ParamField>

    <ParamField query="limit" type="integer" default="10">
      Maximum number of articles to return.
    </ParamField>
    </Accordion>

    This endpoint allows you to retrieve articles with various filtering options. It supports both text-based and semantic search, and can filter articles based on the presence of embeddings, geocoding, entities, and classifications.

    The response will include detailed information about each article, including its content, associated entities, tags, and classification (if available).
  </Step>
</Steps>
</Card>
### Adding Sources
<Card>
<Steps>
  <Step title="Create Scraping/ Sourcing Script">
    To add a new source to SSARE, create a scraping script in the scraper_service/scrapers folder.
    That doesn't necessarily need to be scraping script, it can also be just a function where you load data e.g. from an S3 Storage Bucket.
  </Step>
  <Step title="Define Output">
    Ensure your script outputs a DataFrame with the following columns:

    > | url | headline | paragraphs | source |

    <CodeGroup>
```python Example Scraper
import requests
from bs4 import BeautifulSoup
import pandas as pd
def scrape_cnn_articles():
# Scraping logic here
# Save with the following columns:
# url
# headline
# paragraphs
# source
# ...
df = pd.DataFrame(articles)
return df
cnn_articles_df = scrape_cnn_articles()
cnn_articles_df.to_csv('cnn_articles.csv', index=False)
```
</CodeGroup>
  </Step>
  <Step title="Integrate with SSARE">
    Add your new scraper to the SSARE pipeline for automatic processing.
  </Step>
</Steps>
</Card>




## Use Cases

### Entity Ranking

Retrieve and rank entities from your processed articles:

<CodeGroup>
```python Entity Ranking Script
import requests
from collections import Counter, defaultdict

def print_sorted_entities(entity_type, limit=10):
    url = 'http://localhost:5434/articles'
    params = {
        'geocoding_created': 0,
        'limit': 200,
        'embeddings_created': 1,
        'entities_extracted': 1
    }

    response = requests.get(url, params=params)
    if response.status_code == 200:
        data = response.json()
        entity_counter = Counter()
        entity_articles = defaultdict(list)

        for article in data:
            entities = article['entities']
            for entity in entities:
                if entity['tag'] == entity_type:
                    entity_name = entity['text']
                    entity_counter[entity_name] += 1
                    entity_articles[entity_name].append(article['headline'] or article['url'])

        for entity, count in entity_counter.most_common(limit):
            print(f"{entity_type}: {entity}, Count: {count}")
            print("Associated Articles:")
            for article in set(entity_articles[entity]):
                print(f" - {article}")
            print("\n")
    else:
        print('API request failed.')

print_sorted_entities('NORP', 5)  # Print top 5 NORP (affiliation) entities
```
</CodeGroup>

### GeoJSON Generation

Create GeoJSON features from the locations in your data:

<CodeGroup>
```python GeoJSON Generation
import requests
import json
def generate_geojson():
url = 'http://localhost:5434/articles'
params = {
'geocoding_created': 1,
'limit': 100
}
response = requests.get(url, params=params)
if response.status_code == 200:
    data = response.json()
    features = []

    for article in data:
        for entity in article['entities']:
            if entity['tag'] == 'GPE' and 'lat' in entity and 'lon' in entity:
                feature = {
                    "type": "Feature",
                    "geometry": {
                        "type": "Point",
                        "coordinates": [entity['lon'], entity['lat']]
                    },
                    "properties": {
                        "name": entity['text'],
                        "article": article['headline']
                        # add any other information you want to include in the geojson
                    }
                }
                features.append(feature)

    geojson = {
        "type": "FeatureCollection",
        "features": features
    }

    with open('locations.geojson', 'w') as f:
        json.dump(geojson, f)

    print("GeoJSON file created: locations.geojson")
else:
    print('API request failed.')
generate_geojson()
```
</CodeGroup>

## Roadmap

<CardGroup cols={2}>
  <Card title="Custom Embedding Models" icon="cube">
    Support for user-defined embedding models
  </Card>
  <Card title="Enhanced Geocoding" icon="map">
    Improve accuracy and coverage of location data
  </Card>
  <Card title="Kubernetes Orchestration" icon="ship">
    Scalable deployment with Kubernetes
  </Card>
  <Card title="Expanded Scraper Support" icon="spider">
    We are looking forward to creating "flavors" of information spaces. This will need flexible and modular scraping.
  </Card>
  <Card title="Knowledge Graphs" icon="diagram-project">
    Implement knowledge graph capabilities for enhanced data relationships
  </Card>
  <Card title="GraphRAG" icon="chart-network">
    Integrate Graph Retrieval-Augmented Generation for improved context understanding
  </Card>
  <Card title="Custom Information Spaces" icon="globe">
    Our initial focus is on international politics and global news. 
    We aim to expand this to individual information spaces for more granular coverage. 
    We're also working towards multi-region and multi-language support.
  </Card>
</CardGroup>

## Contributing

We welcome contributions from developers, data scientists, and political enthusiasts. To contribute:

1. Fork the repository
2. Create a new branch for your feature
3. Commit your changes
4. Open a pull request

For major changes, please open an issue first to discuss what you would like to change.

## License

SSARE is distributed under the MIT License. See the `LICENSE` file in the repository for full details.
