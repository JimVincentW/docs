---
title: 'Core Data Pipeline'
description: 'How OPOL processes political information'
---

The OPOL Data Pipeline transforms raw political content into structured, enriched data ready for analysis. This page explains how information flows through the system, from initial collection to final availability in the platform.

<Frame caption="OPOL Data Pipeline Overview">
  <img src="/images/data_pipeline_overview.png" alt="Data Pipeline Overview" />
</Frame>

## Pipeline Stages

Political information moves through several processing stages, each adding value and structure:

<Steps>
  <Step title="Content Collection">
    The system gathers raw content from diverse sources
  </Step>
  
  <Step title="Text Processing">
    Content is cleaned, normalized, and prepared for analysis
  </Step>
  
  <Step title="Entity Extraction">
    People, organizations, and locations are identified
  </Step>
  
  <Step title="Geocoding">
    Location entities are converted to geographic coordinates
  </Step>
  
  <Step title="Embedding Generation">
    Content is transformed into vector representations
  </Step>
  
  <Step title="Classification">
    User-defined analytical frameworks are applied
  </Step>
  
  <Step title="Indexing">
    Processed content is made available for search and analysis
  </Step>
</Steps>

Let's explore each stage in detail.

## Content Collection

<Frame caption="Content Collection Flow">
  <img src="/images/scraping_flow.png" alt="Content Collection Flow" />
</Frame>

The pipeline begins with gathering content from diverse sources:

<CardGroup cols={2}>
  <Card title="News Scraping" icon="newspaper">
    Collection of news articles from configured sources through:
    - RSS feeds
    - HTML scraping
    - News APIs
  </Card>
  
  <Card title="Legislative Data" icon="gavel">
    Integration with parliamentary and legislative data:
    - Bills and proposals
    - Voting records
    - Committee proceedings
  </Card>
  
  <Card title="Economic Data" icon="chart-line">
    Collection of economic indicators:
    - OECD datasets
    - National statistics
    - Financial data
  </Card>
  
  <Card title="User Uploads" icon="upload">
    Processing of user-provided documents:
    - PDFs
    - Word documents
    - Text files
  </Card>
</CardGroup>

### Adding Sources

The system supports adding new content sources through configuration:

```python
# Example source configuration
source = {
    "name": "Example News",
    "url": "https://example-news.com/feed",
    "type": "rss",
    "language": "en",
    "country": "US",
    "update_frequency": 60  # minutes
}
```

<Note>
  For detailed information on adding new sources, see the [Source Configuration Guide](/engine/source-configuration).
</Note>

## Text Processing

Raw content is processed into a clean, structured format:

<AccordionGroup>
  <Accordion title="Content Extraction">
    For web sources, the system:
    - Identifies and extracts the main content
    - Removes advertisements and boilerplate
    - Preserves essential structure (headlines, paragraphs)
    
    <Info>
      We use a combination of rule-based extraction and machine learning approaches adapted to different source formats.
    </Info>
  </Accordion>
  
  <Accordion title="Normalization">
    Text is normalized through:
    - Encoding standardization (UTF-8)
    - Whitespace normalization
    - Special character handling
    - Language detection and tagging
  </Accordion>
  
  <Accordion title="Metadata Extraction">
    The system extracts and structures metadata:
    - Publication date and time
    - Author information
    - Source details
    - Content categories
    - Tags and keywords
  </Accordion>
  
  <Accordion title="Document Structuring">
    Content is structured into a standard format:
    ```python
    document = {
        "id": "unique-identifier",
        "title": "Article Headline",
        "source": "Source Name",
        "url": "https://original-url.com/article",
        "published_date": "2023-06-15T14:30:00Z",
        "language": "en",
        "content": "Full text content...",
        "paragraphs": ["Paragraph 1", "Paragraph 2", ...],
        "metadata": {...}
    }
    ```
  </Accordion>
</AccordionGroup>

## Entity Extraction

<Frame caption="Entity Extraction Process">
  <img src="/images/entity_extraction.png" alt="Entity Extraction Process" />
</Frame>

The system identifies and categorizes named entities mentioned in the content:

<CardGroup cols={3}>
  <Card title="People" icon="user">
    Individuals mentioned in the content:
    - Politicians
    - Business leaders
    - Public figures
  </Card>
  
  <Card title="Organizations" icon="building">
    Organizational entities:
    - Government bodies
    - Companies
    - Political parties
    - NGOs
  </Card>
  
  <Card title="Locations" icon="map-marker">
    Geographic references:
    - Countries
    - Cities
    - Regions
    - Specific locations
  </Card>
</CardGroup>

### Entity Extraction Methods

The system uses two complementary approaches to entity extraction:

<Tabs>
  <Tab title="LLM-based Extraction">
    Using language models for complex entity understanding:
    
    - Better at handling contextual entities
    - Understands implicit references
    - Processes complex relationships
    - Works across multiple languages
    
    <Note>
      LLM extraction is more computationally intensive but provides higher quality for complex cases.
    </Note>
  </Tab>
  
  <Tab title="NER Models">
    Using specialized Named Entity Recognition models:
    
    - Faster processing
    - Lower computational requirements
    - Reliable for standard entity types
    - Language-specific models
    
    <Note>
      NER models are used for high-volume processing where speed is important.
    </Note>
  </Tab>
</Tabs>

### Entity Storage

Extracted entities are stored with rich contextual information:

```python
entity = {
    "id": "entity-uuid",
    "name": "Entity Name",
    "type": "PERSON",  # or ORGANIZATION, LOCATION, etc.
    "mentions": [
        {
            "document_id": "doc-123",
            "context": "Text surrounding the mention...",
            "position": [start, end],
            "confidence": 0.95
        },
        # Additional mentions...
    ],
    "metadata": {
        # Entity-specific metadata
    }
}
```

## Geocoding

<Frame caption="Geocoding Process">
  <img src="/images/geocoding_process.png" alt="Geocoding Process" />
</Frame>

Location entities are enriched with geographic coordinates:

<Steps>
  <Step title="Location Normalization">
    Location names are standardized and ambiguities resolved
  </Step>
  
  <Step title="Geocode Lookup">
    Locations are matched to coordinates using:
    - Local Pelias geocoder
    - External geocoding services (if configured)
  </Step>
  
  <Step title="Contextual Resolution">
    Ambiguous locations are resolved using document context
  </Step>
  
  <Step title="Geographic Enrichment">
    Additional data is added:
    - Region and country information
    - Administrative boundaries
    - Population data
    - Geographic relationships
  </Step>
</Steps>

### Geocoding Implementation

The system uses a multi-tier approach to geocoding:

<AccordionGroup>
  <Accordion title="Local Geocoding">
    For offline and privacy-preserving operation:
    
    - Uses Pelias Placeholder for location name resolution
    - Pre-loaded geographic database
    - Fast lookup for common locations
    - No external API dependencies
  </Accordion>
  
  <Accordion title="External Services">
    For enhanced coverage (when configured):
    
    - OpenStreetMap Nominatim
    - Google Maps (if API key provided)
    - Other configurable geocoding services
  </Accordion>
  
  <Accordion title="Custom Location Database">
    For specialized political entities:
    
    - Legislative districts
    - Non-standard political boundaries
    - Historical regions
    - Disputed territories
  </Accordion>
</AccordionGroup>

## Embedding Generation

<Frame caption="Vector Embedding Process">
  <img src="/images/embedding_process.png" alt="Vector Embedding Process" />
</Frame>

Content is transformed into numerical vector representations:

<CardGroup cols={2}>
  <Card title="Document Embeddings" icon="file-alt">
    Vector representations of entire documents for:
    - Semantic search
    - Document clustering
    - Similarity analysis
  </Card>
  
  <Card title="Paragraph Embeddings" icon="paragraph">
    Fine-grained embeddings for:
    - Passage retrieval
    - Detailed analysis
    - Section comparison
  </Card>
</CardGroup>

### Embedding Methods

The system supports multiple embedding models:

<Tabs>
  <Tab title="Local Embeddings">
    Using Ollama for local processing:
    
    - Complete data privacy
    - No external API costs
    - Customizable models
    - Works offline
    
    <Info>
      Default setup uses `nomic-embed-text` through Ollama
    </Info>
  </Tab>
  
  <Tab title="OpenAI Embeddings">
    Using OpenAI's embedding models:
    
    - High quality semantic representations
    - Fast processing through API
    - Regular model updates
    
    <Info>
      Requires an OpenAI API key configured in `.env.local`
    </Info>
  </Tab>
  
  <Tab title="Custom Embeddings">
    Support for other embedding models:
    
    - HuggingFace models
    - Google Vertex AI
    - Self-hosted options
    
    <Info>
      Configure alternative models in `.env.local`
    </Info>
  </Tab>
</Tabs>

### Vector Storage

Embeddings are stored in a PostgreSQL database with pgvector:

- Fast similarity search via indexes
- Integration with metadata filtering
- Efficient storage and retrieval
- Transparent to application code

## Classification

<Frame caption="Classification Process">
  <img src="/images/classification_process.png" alt="Classification Process" />
</Frame>

The system applies user-defined analytical frameworks to content:

<Steps>
  <Step title="Schema Definition">
    Users define analytical dimensions in natural language
  </Step>
  
  <Step title="Context Preparation">
    Content is prepared with appropriate context for analysis
  </Step>
  
  <Step title="LLM Processing">
    Language models extract structured information based on schemas
  </Step>
</Steps>