---
title: 'Getting Started'
description: 'Get up and running with Open Politics HQ'
---

# Getting Started

You have two options: use our hosted instance, or run HQ yourself.

## Choose Your Setup

<Tabs>
<Tab title="Hosted Instance">
The fastest way to start. We handle the infrastructure - you bring your own LLM API keys.

<Steps>
<Step title="Register">
Sign up at [open-politics.org](https://open-politics.org/accounts/register)
</Step>
<Step title="Add API Keys">
Configure your LLM API keys (OpenAI, Anthropic, Google, or run Ollama locally)
</Step>
<Step title="Start Analysing">
Upload content and create your first schema
</Step>
</Steps>

<Note>
Your account also works on the [forum](https://forum.open-politics.org) for community support.
</Note>
</Tab>

<Tab title="Self-Hosted">
For privacy, customization, or institutional requirements. Run everything on your own infrastructure.

<Steps>
<Step title="Prerequisites">
Docker, Git, 32GB RAM recommended
</Step>
<Step title="Clone & Setup">
```bash
git clone https://github.com/open-politics/open-politics-hq.git
cd open-politics-hq
cp .env.example .env
# Edit .env with your configuration
```
</Step>
<Step title="Start Services">
```bash
docker compose up
```
</Step>
<Step title="Access Platform">
Open http://localhost:3000
</Step>
</Steps>

<Warning>
**Change the defaults immediately:**
- Username: `app_user`
- Password: `app_user_password`
</Warning>

### Deployment Options

<CardGroup cols={3}>
  <Card title="Fully Local" icon="shield">
    Everything on your hardware. Air-gapped environments, complete data control.
  </Card>
  <Card title="Hybrid" icon="cloud">
    App locally, managed services (AWS RDS, Upstash Redis, S3) for less overhead.
  </Card>
  <Card title="Kubernetes" icon="server">
    Helm chart available at [GitHub](https://github.com/open-politics/open-politics-hq/tree/main/.deployments)
  </Card>
</CardGroup>
</Tab>
</Tabs>

---

## LLM Support

HQ works with multiple AI providers. Bring your own keys.

<CardGroup cols={2}>
  <Card title="Ollama" icon="shield" color="#0D9373">
    Run locally: Llama, Qwen, etc. - complete privacy
  </Card>
  <div>
  <Card title="Anthropic" icon="brain" color="#EC4899">
    Claude models - best experience with tools
  </Card>
  <Card title="OpenAI" icon="robot" color="#10A37F">
    GPT-4o and newer
  </Card>
  <Card title="Google" icon="google" color="#4285F4">
    Gemini - large context, speed, multimodal
  </Card>
  </div>
</CardGroup>

Configure keys in the web interface. Or run Ollama locally and skip the cloud entirely.